{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "374cd9f8-b729-4f45-a1ee-77c548380bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import os\n",
    "import io\n",
    "# to download data\n",
    "import wget\n",
    "# storing and analysing data\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "\n",
    "class DownloadData():\n",
    "    \n",
    "    def __init__(self, file, folder_csv, folder_source):\n",
    "        self.file          = file\n",
    "        self.folder_csv    = folder_csv\n",
    "        self.folder_source = folder_source\n",
    "\n",
    "    def delete_file(self):\n",
    "        if len(os.listdir(self.folder_csv)) > 0:\n",
    "            for file in os.listdir(self.folder_csv):\n",
    "                if file[0] != \".\":\n",
    "                    os.remove(os.path.join(self.folder_csv, file))\n",
    "        return \"Delete File\"\n",
    "        \n",
    "\n",
    "    def download_file(self):\n",
    "        # download files\n",
    "        for url in self.file:\n",
    "            filename = wget.download(url)\n",
    "            os.replace(folder_source+filename, folder_csv+filename)\n",
    "        return \"Download file\"\n",
    "\n",
    "\n",
    "\n",
    "class TransformFile(DownloadData):\n",
    "\n",
    "\n",
    "    def __init__(self,spark_session, folder_parquet,folder_csv=None, folder_source=None, file=None):\n",
    "        super().__init__(file, folder_csv, folder_source)\n",
    "        self.spark_session   = spark_session\n",
    "        self.folder_parquet  = folder_parquet\n",
    "\n",
    "\n",
    "    def csv_to_parquet(self):\n",
    "        try:\n",
    "            if len(os.listdir(folder_csv)) > 0:\n",
    "                for file in os.listdir(folder_csv):\n",
    "                    if file[0] != \".\":\n",
    "                        print(file)\n",
    "                        pandas_df = pd.read_csv(folder_csv + file)\n",
    "                        # Convierte el DataFrame de pandas a un DataFrame de Spark\n",
    "                        spark_df = self.spark_session.createDataFrame(pandas_df)\n",
    "                        # Llena los valores nulos con un valor predeterminado (ajusta seg√∫n tus necesidades)\n",
    "                        spark_df = spark_df.fillna(\"Null\")\n",
    "                        # Guarda el DataFrame como un archivo Parquet en el directorio especificado\n",
    "                        spark_df.write.mode(\"overwrite\").parquet(folder_parquet)\n",
    "                        \n",
    "            return \"Transform\"\n",
    "        except Exception as e:\n",
    "            return f\"Error {e}\"\n",
    "\n",
    "\n",
    "\n",
    "class ChangeColumnType():\n",
    "\n",
    "    def __init__(self, folder_parquet):\n",
    "        self.folder_parquet = folder_parquet\n",
    "           \n",
    "\n",
    "    def update_parquet(self):\n",
    "        try:\n",
    "            for file in os.listdir(self.folder_parquet):\n",
    "                if file[0] != \".\" and file[0] != \"_\":\n",
    "                    df_parquet = pd.read_parquet(self.folder_parquet+file)\n",
    "                    columns_int = df_parquet.select_dtypes(include=['int', 'float']).columns\n",
    "                    df_parquet[columns_int] = df_parquet[columns_int].astype(object)\n",
    "                    df_parquet.to_parquet(self.folder_parquet+file, index=True, compression=\"snappy\", engine='auto')\n",
    "                    df_parquet = pd.read_parquet(self.folder_parquet+file)\n",
    "            return df_parquet.dtypes\n",
    "        except Exception as e:\n",
    "            return f\"Error al leer el archivo Parquet: {e}\"\n",
    "\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c265f7d9-5f83-47dc-b862-f31d809c8245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [............................................................................] 159506 / 159506time_series_covid19_confirmed_global.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n",
      "23/12/03 22:32:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/12/03 22:32:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/12/03 22:32:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/12/03 22:32:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/12/03 22:32:56 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who_covid_19_sit_rep_time_series.csv\n",
      "time_series_covid19_recovered_global.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n",
      "23/12/03 22:32:59 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/12/03 22:32:59 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/12/03 22:32:59 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "23/12/03 22:32:59 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "23/12/03 22:32:59 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID_ISO_FIPS_LookUp_Table.csv\n",
      "time_series_covid19_deaths_global.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Spark Session\n",
    "    spark = SparkSession.builder.appName(\"N5ProcesadorDeDatos\").getOrCreate()\n",
    "\n",
    "    #Url from process\n",
    "    data_time_series      = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/\"\n",
    "    data_iso_fips         = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/\"\n",
    "    data_situation_report = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/who_covid_19_situation_reports/who_covid_19_sit_rep_time_series/\"\n",
    "\n",
    "    #Path folders for files\n",
    "    folder_source = \"\"\n",
    "    folder_csv = \"\"\n",
    "    folder_parquet = \"\"\n",
    "\n",
    "    #File to Process\n",
    "    urls = [data_time_series+'time_series_covid19_confirmed_global.csv', \n",
    "            data_time_series+'time_series_covid19_deaths_global.csv',\n",
    "            data_time_series+'time_series_covid19_recovered_global.csv',\n",
    "            data_iso_fips+'UID_ISO_FIPS_LookUp_Table.csv',\n",
    "            data_situation_report+'who_covid_19_sit_rep_time_series.csv'\n",
    "            ]\n",
    "    \n",
    "    #Download Data from Github\n",
    "    data = DownloadData( urls, folder_csv, folder_source)\n",
    "    delete_file = data.delete_file()\n",
    "    get_data = data.download_file()\n",
    "\n",
    "    #Process file csv to parquet\n",
    "    process = TransformFile(spark ,folder_parquet)\n",
    "    process_file = process.csv_to_parquet()\n",
    "    print( process_file)\n",
    "\n",
    "    #Update parquet\n",
    "    update = ChangeColumnType(folder_parquet)\n",
    "    updtae_column = update.update_parquet()\n",
    "    \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44367a4f-1d96-4e27-8814-b9ed62adc8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e5f12-b112-4de6-bf57-6037429c4f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
